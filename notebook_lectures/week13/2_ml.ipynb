{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 14 Day 2: Intro to Machine Learning\n",
    "\n",
    "## Objectives\n",
    "* Cover basic terminology of Machine Learning\n",
    "* Look at a Logistic Regression example (the Hello World of DL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML terminology\n",
    "\n",
    "See class notes for diagrams and descriptions.\n",
    "\n",
    "* **Training sample**: The data you use to train the network.\n",
    "* **Validation sample**: An independent data sample you can test the network with.\n",
    "* **Overtraining**: Learning details of the training sample instead of the underlying distribution.\n",
    "* **Weights**: The parameters you fit to.\n",
    "* **Layers**: A way to group the distribution definition. Named for the (usually linear) function that produces them:\n",
    "    - *Fully Connected linear layer (FC)*: A simple matrix of weights\n",
    "    - *Convolutional layer*: A layer that relates nearby data structures\n",
    "    - *Recurrent layer*: A layer that has some \"memory\" - often used for variable length data.\n",
    "* **Hidden layer**: An \"inbetween\" state not externally visible.\n",
    "* **Activation function**: A non-linear function that can applies after a layer.\n",
    "    * *ReLu*: Rectified linear unit function\n",
    "    * *Sigmoid*: Maps $(-\\infty,\\infty) \\rightarrow (0,1)$\n",
    "* **Network**: The collection of weights in layers and activation functions. Also called a model.\n",
    "* **Loss function**: A function that gives you a \"score\" for how poorly you did.\n",
    "* **Cost function**: Sum of the loss function over your training sample.\n",
    "* **Batch**: A smaller division used for evaluating data\n",
    "* **Epoch**: An iteration over the whole training sample (one \"step\")\n",
    "* **Backpropagation**: Taking the derivative of the network\n",
    "* **Learning rate**: How far to move the weights based on the gradient after each epoch.\n",
    "* **Neuron**: The combination of a weight and an activation function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network terminology\n",
    "\n",
    "* **Deep learning**: Large neural networks with hidden layers\n",
    "* **CNNs**: *Convolutional Neural* Nets: Looks for spacial structures, like in images\n",
    "* **RNNs**: *Recurrent Neural Nets*: Have some form of memory (Recursive NN are one form of RNN, among others)\n",
    "* **GANs**: *Generative Adversarial Nets*: can run in reverse to generate possible inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple network\n",
    "\n",
    "See the excellent tutorials here: [PyTorch with examples](https://pytorch.org/tutorials/beginner/pytorch_with_examples.html) for examples using Numpy, Torch, and TensorFlow.\n",
    "\n",
    "Let's start with a batch of random numbers - this will be our \"data\". We will do the following:\n",
    "\n",
    "```\n",
    "1,000 x N     1000 x 100          100 x 10     10 x N\n",
    " data (x)         ->       ReLu       ->     result (y)\n",
    "               fully connected linear layers\n",
    "```\n",
    "\n",
    "In words: we have N samples of data with `D_in = 1000` values each. We convert that to `H = 100` values, use a relu activation function, then convert that to `D_out = 10` values and compare with the expected result.\n",
    "\n",
    "Since this is all random, we can train on small samples because we have so many parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 64               # batch size\n",
    "D_in = 1_000         # input dimension\n",
    "H = 100              # hidden dimension\n",
    "D_out = 10           # output dimension\n",
    "\n",
    "epochs = 500         # How many iterations to run\n",
    "learning_rate = 1e-6 # How much to move each iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomly initializing weights is important in some cases, though in this one it is a bit harder to show (IMO)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Create random input and output data\n",
    "x = np.random.randn(N, D_in)\n",
    "y = np.random.randn(N, D_out)\n",
    "\n",
    "# Randomly initialize weights\n",
    "w1 = np.random.randn(D_in, H)\n",
    "w2 = np.random.randn(H, D_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's loop and do the calculations.\n",
    "\n",
    "The derivatives are easy - just look at our definitions:\n",
    "\n",
    "$$\n",
    "b = x \\cdot w_1 \\tag{1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "h = \\mathrm{ReLu}(b) \\tag{2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{y} = h \\cdot w_2 \\tag{3}\n",
    "$$\n",
    "\n",
    "$$\n",
    "L(\\hat{y}, y) = \\left(\\hat{y} - y\\right)^2 \\tag{4}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The derivatives:\n",
    "\n",
    "$$\n",
    "\\frac{dL}{d\\hat{y}} = 2  \\left(\\hat{y} - y\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{d L}{d w_2} = h^T \\cdot \\frac{dL}{d\\hat{y}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{d L}{d h} = \\frac{dL}{d\\hat{y}} \\cdot w_2^T\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{d L}{d b} = \\mathrm{ReLu}\\left(\\frac{d L}{d h}\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{d L}{d w_1} = x^T \\cdot \\frac{d L}{d b}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(epochs):\n",
    "    \n",
    "    # Forward pass: compute predicted y\n",
    "    h = x @ w1                 # First FC layer          (1)\n",
    "    h_relu = np.maximum(h, 0)  # Activation function     (2)\n",
    "    y_pred = h_relu @ w2       # Second FC layer         (3)\n",
    "\n",
    "    # Compute and print cost\n",
    "    cost = np.sum((y_pred - y)**2) # Sum of loss         (4)\n",
    "    print(t, cost)\n",
    "\n",
    "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)     #          back (4)\n",
    "    grad_w2 = h_relu.T @ grad_y_pred     #          dL/dw2\n",
    "    grad_h_relu = grad_y_pred @ w2.T     #          back (3)\n",
    "    grad_h = np.maximum(grad_h_relu, 0)  #          back (2)\n",
    "    grad_w1 = x.T @ grad_h               #          dL/dw1\n",
    "    \n",
    "    # Update weights\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
