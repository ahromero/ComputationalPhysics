{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 9 Day 2: Markov Chain Monte Carlo\n",
    "\n",
    "## Objectives\n",
    "\n",
    "* Set up a new virtual environment\n",
    "* Broad overview of MCMC\n",
    "* Look at PyMC3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Virtual environments\n",
    "### Or how to install a conda package properly\n",
    "\n",
    "We are going to need to install Theano on OSC. This means we need a new conda virtual environment! Let's see how we would do that. I would open a new terminal, because it's a bit slow:\n",
    "\n",
    "```bash\n",
    "conda create -n theano_env python=3.6 anaconda theano\n",
    "```\n",
    "\n",
    "Name (`-n`): `theano_env`\n",
    "Python version: 3.6\n",
    "Package list: All packages from `anaconda`, `theano`\n",
    "\n",
    "This will also install up-to-date packages to link into the new environment, so it might be a bit slow. On your local machine, it might be faster, since you probably `conda update anaconda` occasionally.\n",
    "\n",
    "You'll next need your kernels available - you can either add `nb_conda_kernels` to your base environment if you own it (not on OSC), or install them manually:\n",
    "\n",
    "```bash\n",
    "source activate theano_env\n",
    "python -m ipykernel install --user --name theano_env\n",
    "```\n",
    "\n",
    "> Note: In the latest Conda release, besides Python 3.7, you can now use `conda activate` instead.\n",
    "\n",
    "While you are in the environment, go ahead and install pymc3: `pip install pymc3` or `conda install -c conda-forge pymc3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numpy keeps throwing a Future warning, but we will ignore it.\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "import scipy.special\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Biased coin\n",
    "\n",
    "* Based on <http://www.behind-the-enemy-lines.com/2008/01/are-you-bayesian-or-frequentist-or.html>\n",
    "\n",
    "This is a very popular example for understanding Bayesian statistics. Let's assume you have a coin and you think it may be biased. You job is to flip the coin and calculate the bias $p$, where $p$ is the chance of obtaining a head ($p=.5$ for an unbiased coin).\n",
    "\n",
    "You flip the coin 14 times. It lands heads up 10 times.\n",
    "\n",
    "Let's illustrate the difference by asking both the Frequentist and the Bayesian a question: Would you bet even money that the next two flips will both land heads up?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 14\n",
    "heads = 10\n",
    "a = 1\n",
    "b = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequentist statistics\n",
    "\n",
    "The Frequentist simply calculates the value of $p$ given the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_freq = heads / n\n",
    "prob_2_heads = p_freq ** 2\n",
    "print(f\"p={p_freq:.3}, and the chance of two heads is {prob_2_heads:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian statistics: Analytical\n",
    "\n",
    "For us, $p$ is no longer a value, but a distribution. What is the probability of $p$, given data $x$? This is the Bayes formula:\n",
    "\n",
    "$$\n",
    "P(p|x) = \\frac{P(x|p) P(p)}{P(x)} \\tag{1}\n",
    "$$\n",
    "\n",
    "We can compute $P(x|p)$ using a binomial distribution:\n",
    "\n",
    "$$\n",
    "P(x|p) \\propto p^{10} \\left( 1-p \\right)^4\n",
    "$$\n",
    "\n",
    "Bayesian's need to select a prior distribution $P(p)$. We'll pick the Beta distribution:\n",
    "\n",
    "$$\n",
    "P(p) \\propto p^{a-1} \\left( 1-p \\right)^{b-1}\n",
    "$$\n",
    "\n",
    "This gives us a combined distribution of:\n",
    "\n",
    "$$\n",
    "P(p|x) \\propto p^{10+a-1} \\left( 1-p \\right)^{4+b-1}\n",
    "$$\n",
    "\n",
    "Now, we need to normalize (we've been dropping constants everywhere intentionally). This is again a Beta distribution, so we can just write it that way:\n",
    "\n",
    "$$\n",
    "P(p|x) = \\mathrm{Beta}(p; a+10, b+4)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 1, 200)\n",
    "orig = scipy.stats.beta.pdf(x, a, b)\n",
    "new = scipy.stats.beta.pdf(x, a + heads, b + n - heads)\n",
    "plt.plot(x, orig, label=\"Prior\")\n",
    "plt.plot(x, new, label=\"Posterior\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working through some math, we could find the probability of two heads in a row:\n",
    "\n",
    "$$\n",
    "P(HH|x) = \\frac{B(10+a+2, 4+b)}{B(10+a, 4+b)}\n",
    "$$\n",
    "\n",
    "(Here $B$ is the beta function, not beta distribution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(scipy.special.beta(10 + 1 + 2, 4 + 1) / scipy.special.beta(10 + 1, 4 + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the Bayesian would vote against two heads in a row."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We were lucky here, due to our careful choice of problem. We had a nice posterior and could do everything analytically. What if we didn't such a perfect setup?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov-Chain Monte Carlo\n",
    "\n",
    "See also <https://twiecki.github.io/blog/2015/11/10/mcmc-sampling/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look again at the Bayes formula for a general parameter $\\theta$:\n",
    "\n",
    "$$\n",
    "P(\\theta|x) = \\frac{P(x|\\theta) P(\\theta)}{P(x)} \\tag{1}\n",
    "$$\n",
    "\n",
    "* $P(\\theta)$: Prior\n",
    "* $P(x|\\theta)$: Likelihood\n",
    "* $P(x)$: Evidence\n",
    "\n",
    "We need to compute:\n",
    "\n",
    "$$\n",
    "P(x) = \\int _\\Theta P(x,\\theta) d\\theta\n",
    "$$\n",
    "\n",
    "For non-simple models, this is not closed form. So how about using MC and sampling the posterior? Now you need to solve *and* invert the Bayes formula! We didn't fix anything.\n",
    "\n",
    "How about we go crazy and decide to build a markov chain that has an equilibrium state that matches our unknown posterior distribution? While this may seem yet one step harder still, it turns out that we can do it rather easily!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Chains\n",
    "\n",
    "Let's change gears just for a second, and talk about Markov chains. A Markov chain is a system where the next state of the system depends *only* on the current state of the system, not on any prior states. Chutes and Ladders, the children's board game, is the canonical example: you can set the game up, then the next possible state is a collection of probabilities based on dice rolls and the current marker locations, but it does not have any \"memory\" as to how you got to the current state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For us, it means that we start at some point, then try to \"jump\" to a next point based on a probability. If we like the new point better, we keep it. This would be a simple search, and would \"stop\" at the peak -- but we want a distribution, not a value -- so we make an acceptance probability: $p_\\mathrm{accept} = p_\\mathrm{proposal} / p_\\mathrm{current}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyMC3\n",
    "\n",
    "Let's look at MCMC to solve the coin problem above. We'll adjust our numbers a bit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "heads = 61\n",
    "a = 10\n",
    "b = 10\n",
    "niter = 2_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 1, 200)\n",
    "orig = scipy.stats.beta.pdf(x, a, b)\n",
    "new = scipy.stats.beta.pdf(x, a + heads, b + n - heads)\n",
    "plt.plot(x, orig, label=\"Prior\")\n",
    "plt.plot(x, new, label=\"Posterior\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc3 as pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as coin_context:\n",
    "    p = pm.Beta(\"p\", alpha=2, beta=2)\n",
    "    y = pm.Binomial(\"y\", n=n, p=p, observed=heads)\n",
    "    trace = pm.sample(niter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(trace[\"p\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = trace.get_values(\"p\", burn=niter // 2, combine=True, chains=[0, 2])\n",
    "plt.plot(p)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian example\n",
    "\n",
    "Taken verbatim from <https://twiecki.github.io/blog/2015/11/10/mcmc-sampling/>. It uses a Gaussian instead of the coin flip. We are estimating the mean from 20 samples drawn from a data sample centered around 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.stats import norm\n",
    "\n",
    "sns.set_style(\"white\")\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.random.randn(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.subplot()\n",
    "sns.distplot(data, kde=False, ax=ax)\n",
    "_ = ax.set(title=\"Histogram of observed data\", xlabel=\"x\", ylabel=\"# observations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_posterior_analytical(data, x, mu_0, sigma_0):\n",
    "    sigma = 1.0\n",
    "    n = len(data)\n",
    "    mu_post = (mu_0 / sigma_0 ** 2 + data.sum() / sigma ** 2) / (\n",
    "        1.0 / sigma_0 ** 2 + n / sigma ** 2\n",
    "    )\n",
    "    sigma_post = (1.0 / sigma_0 ** 2 + n / sigma ** 2) ** -1\n",
    "    return norm(mu_post, np.sqrt(sigma_post)).pdf(x)\n",
    "\n",
    "\n",
    "ax = plt.subplot()\n",
    "x = np.linspace(-1, 1, 500)\n",
    "posterior_analytical = calc_posterior_analytical(data, x, 0.0, 1.0)\n",
    "ax.plot(x, posterior_analytical)\n",
    "ax.set(xlabel=\"mu\", ylabel=\"belief\", title=\"Analytical posterior\")\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampler(\n",
    "    data,\n",
    "    samples=4,\n",
    "    mu_init=0.5,\n",
    "    proposal_width=0.5,\n",
    "    plot=False,\n",
    "    mu_prior_mu=0,\n",
    "    mu_prior_sd=1.0,\n",
    "):\n",
    "    mu_current = mu_init\n",
    "    posterior = [mu_current]\n",
    "    for i in range(samples):\n",
    "        # suggest new position\n",
    "        mu_proposal = norm(mu_current, proposal_width).rvs()\n",
    "\n",
    "        # Compute likelihood by multiplying probabilities of each data point\n",
    "        likelihood_current = norm(mu_current, 1).pdf(data).prod()\n",
    "        likelihood_proposal = norm(mu_proposal, 1).pdf(data).prod()\n",
    "\n",
    "        # Compute prior probability of current and proposed mu\n",
    "        prior_current = norm(mu_prior_mu, mu_prior_sd).pdf(mu_current)\n",
    "        prior_proposal = norm(mu_prior_mu, mu_prior_sd).pdf(mu_proposal)\n",
    "\n",
    "        p_current = likelihood_current * prior_current\n",
    "        p_proposal = likelihood_proposal * prior_proposal\n",
    "\n",
    "        # Accept proposal?\n",
    "        p_accept = p_proposal / p_current\n",
    "\n",
    "        # Usually would include prior probability, which we neglect here for simplicity\n",
    "        accept = np.random.rand() < p_accept\n",
    "\n",
    "        if plot:\n",
    "            plot_proposal(\n",
    "                mu_current,\n",
    "                mu_proposal,\n",
    "                mu_prior_mu,\n",
    "                mu_prior_sd,\n",
    "                data,\n",
    "                accept,\n",
    "                posterior,\n",
    "                i,\n",
    "            )\n",
    "\n",
    "        if accept:\n",
    "            # Update position\n",
    "            mu_current = mu_proposal\n",
    "\n",
    "        posterior.append(mu_current)\n",
    "\n",
    "    return posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Function to display\n",
    "def plot_proposal(\n",
    "    mu_current, mu_proposal, mu_prior_mu, mu_prior_sd, data, accepted, trace, i\n",
    "):\n",
    "    from copy import copy\n",
    "\n",
    "    trace = copy(trace)\n",
    "    fig, (ax1, ax2, ax3, ax4) = plt.subplots(ncols=4, figsize=(16, 4))\n",
    "    fig.suptitle(\"Iteration %i\" % (i + 1))\n",
    "    x = np.linspace(-3, 3, 5000)\n",
    "    color = \"g\" if accepted else \"r\"\n",
    "\n",
    "    # Plot prior\n",
    "    prior_current = norm(mu_prior_mu, mu_prior_sd).pdf(mu_current)\n",
    "    prior_proposal = norm(mu_prior_mu, mu_prior_sd).pdf(mu_proposal)\n",
    "    prior = norm(mu_prior_mu, mu_prior_sd).pdf(x)\n",
    "    ax1.plot(x, prior)\n",
    "    ax1.plot([mu_current] * 2, [0, prior_current], marker=\"o\", color=\"b\")\n",
    "    ax1.plot([mu_proposal] * 2, [0, prior_proposal], marker=\"o\", color=color)\n",
    "    ax1.annotate(\n",
    "        \"\",\n",
    "        xy=(mu_proposal, 0.2),\n",
    "        xytext=(mu_current, 0.2),\n",
    "        arrowprops=dict(arrowstyle=\"->\", lw=2.0),\n",
    "    )\n",
    "    ax1.set(\n",
    "        ylabel=\"Probability Density\",\n",
    "        title=\"current: prior(mu=%.2f) = %.2f\\nproposal: prior(mu=%.2f) = %.2f\"\n",
    "        % (mu_current, prior_current, mu_proposal, prior_proposal),\n",
    "    )\n",
    "\n",
    "    # Likelihood\n",
    "    likelihood_current = norm(mu_current, 1).pdf(data).prod()\n",
    "    likelihood_proposal = norm(mu_proposal, 1).pdf(data).prod()\n",
    "    y = norm(loc=mu_proposal, scale=1).pdf(x)\n",
    "    sns.distplot(data, kde=False, norm_hist=True, ax=ax2)\n",
    "    ax2.plot(x, y, color=color)\n",
    "    ax2.axvline(mu_current, color=\"b\", linestyle=\"--\", label=\"mu_current\")\n",
    "    ax2.axvline(mu_proposal, color=color, linestyle=\"--\", label=\"mu_proposal\")\n",
    "    # ax2.title('Proposal {}'.format('accepted' if accepted else 'rejected'))\n",
    "    ax2.annotate(\n",
    "        \"\",\n",
    "        xy=(mu_proposal, 0.2),\n",
    "        xytext=(mu_current, 0.2),\n",
    "        arrowprops=dict(arrowstyle=\"->\", lw=2.0),\n",
    "    )\n",
    "    ax2.set(\n",
    "        title=\"likelihood(mu=%.2f) = %.2f\\nlikelihood(mu=%.2f) = %.2f\"\n",
    "        % (\n",
    "            mu_current,\n",
    "            1e14 * likelihood_current,\n",
    "            mu_proposal,\n",
    "            1e14 * likelihood_proposal,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Posterior\n",
    "    posterior_analytical = calc_posterior_analytical(data, x, mu_prior_mu, mu_prior_sd)\n",
    "    ax3.plot(x, posterior_analytical)\n",
    "    posterior_current = calc_posterior_analytical(\n",
    "        data, mu_current, mu_prior_mu, mu_prior_sd\n",
    "    )\n",
    "    posterior_proposal = calc_posterior_analytical(\n",
    "        data, mu_proposal, mu_prior_mu, mu_prior_sd\n",
    "    )\n",
    "    ax3.plot([mu_current] * 2, [0, posterior_current], marker=\"o\", color=\"b\")\n",
    "    ax3.plot([mu_proposal] * 2, [0, posterior_proposal], marker=\"o\", color=color)\n",
    "    ax3.annotate(\n",
    "        \"\",\n",
    "        xy=(mu_proposal, 0.2),\n",
    "        xytext=(mu_current, 0.2),\n",
    "        arrowprops=dict(arrowstyle=\"->\", lw=2.0),\n",
    "    )\n",
    "    # x3.set(title=r'prior x likelihood $\\propto$ posterior')\n",
    "    ax3.set(\n",
    "        title=\"posterior(mu=%.2f) = %.5f\\nposterior(mu=%.2f) = %.5f\"\n",
    "        % (mu_current, posterior_current, mu_proposal, posterior_proposal)\n",
    "    )\n",
    "\n",
    "    if accepted:\n",
    "        trace.append(mu_proposal)\n",
    "    else:\n",
    "        trace.append(mu_current)\n",
    "    ax4.plot(trace)\n",
    "    ax4.set(xlabel=\"iteration\", ylabel=\"mu\", title=\"trace\")\n",
    "    plt.tight_layout()\n",
    "    # plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "sampler(data, samples=8, mu_init=-1.0, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "posterior = sampler(data, samples=15000, mu_init=1.0)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(posterior)\n",
    "_ = ax.set(xlabel=\"sample\", ylabel=\"mu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.subplot()\n",
    "\n",
    "sns.distplot(posterior[500:], ax=ax, label=\"estimated posterior\")\n",
    "x = np.linspace(-0.5, 0.5, 500)\n",
    "post = calc_posterior_analytical(data, x, 0, 1)\n",
    "ax.plot(x, post, \"g\", label=\"analytic posterior\")\n",
    "_ = ax.set(xlabel=\"mu\", ylabel=\"belief\")\n",
    "ax.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "compclass",
   "language": "python",
   "name": "compclass"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
