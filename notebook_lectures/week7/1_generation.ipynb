{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 8 Day 1: Generating distributions\n",
    "\n",
    "## Objectives:\n",
    "* Talk a bit about projects\n",
    "* Review older homework sets\n",
    "* Talk a bit about testing\n",
    "* Look at generating distributions\n",
    "\n",
    "Reminder: Answers are hidden in the notebook - double click on the text above the cell to get the answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "There are two types of tests:\n",
    "* **Integration tests** - these check your project as a whole\n",
    "* **Unit tests** - these test teach component of your project\n",
    "\n",
    "Unit tests can also have a useful metric:\n",
    "* **Coverage**: how much of your code is run by the unit tests\n",
    "\n",
    "<font color=\"red\">\n",
    "    \n",
    "> Warning: 100% coverage is great, but does not mean you cannot have bugs!\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def square(x):\n",
    "    return x ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert square(2) == 4\n",
    "assert square(0) == 0\n",
    "assert square(-2) == 4\n",
    "# Test for square(\"hi\") == fail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good tests mean:\n",
    "\n",
    "* At least one per function or functionality of your code\n",
    "* Should cover \"normal\" and \"edge case\" inputs\n",
    "* It's okay to throw an error if something is wrong! You can test for that. Silently failing is bad.\n",
    "\n",
    "Testing may even give users an example of how to run the code!\n",
    "\n",
    "![Coder image](AboutCoders-a5-20101003-2113.jpg)\n",
    "\n",
    "> Image credit: About Coders, by Geek and Poke"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing features from simple testing:\n",
    "\n",
    "* Nice reporting of what went wrong\n",
    "* Finishing the tests even with 1+ failures\n",
    "* Ways to mark tests as OK to fail\n",
    "* Easy to combine many test files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing frameworks: Unittest\n",
    "\n",
    "Unittest is built into Python, and is actually pretty powerful (after Python 2.6, anyway). However, it fails at the #1 most important feature of tests: They should be easy to write. Most people will treat tests as something \"optional\", and will not bother to write them unless writing them is fun. Look at the test above, written in Unittest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "\n",
    "\n",
    "class MyTestClass(unittest.TestCase):\n",
    "    def test_square(self):\n",
    "\n",
    "        self.assertEqual(square(2), 4)\n",
    "        self.assertEqual(square(0), 0)\n",
    "        self.assertEqual(square(-2), 4)\n",
    "\n",
    "        self.assertRaises(TypeError, square, \"hi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a file, this is pretty trivial to run, with the auto-test discovery feature. We'll have to use the trick from [here](https://medium.com/@vladbezden/using-python-unittest-in-ipython-or-jupyter-732448724e31) to run in the jupyter notebook, however:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unittest.main(argv=[\"first-arg-is-ignored\"], exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"green\">\n",
    "\n",
    "Pros:\n",
    "\n",
    "* Pure python - can even run from notebook\n",
    "* Included in Python\n",
    "* Somewhat standard \"JUnit\" style\n",
    "\n",
    "</font>\n",
    "<font color=\"red\">\n",
    "\n",
    "Cons:\n",
    "\n",
    "* Ugly and verbose\n",
    "* Have to remember/look up all the comparisons\n",
    "* Have to write JUnit (from Java) style test classes\n",
    "* Even though you are using classes, you *also* have to use names that include the word \"test\"\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing frameworks: Doctest\n",
    "Also built into Python, this looks through your documentation and runs any code it finds! Sounds good, but only useful for very small projects, I've found.\n",
    "\n",
    "## Testing frameworks: Nose\n",
    "Was the first real improvement over Unittest, but was not radically different. Some features made it into Unittest in Python 2.7, and now has mostly been supplanted by PyTest. You might occasionally see Nose out in the wild, but don't use it on your projects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTest\n",
    "\n",
    "This is the \"Pythonic\" answer to testing. It:\n",
    "\n",
    "* Runs Unittest, Nose, and PyTest style tests\n",
    "* Provides a dead-simple interface without classes or special methods\n",
    "* Supports a huge amount of customization, usually in a simple, Pythonic way\n",
    "\n",
    "The downside: it doesn't run inside a Jupyter notebook, since it is actually rewriting python's assert statements!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile pytestexample.py\n",
    "\n",
    "import pytest  # only needed for pytest.raises\n",
    "\n",
    "\n",
    "def square(x):\n",
    "    return x ** 2\n",
    "\n",
    "\n",
    "def test_square():\n",
    "    assert square(2) == 4\n",
    "    assert square(0) == 0\n",
    "    assert square(-2) == 4\n",
    "\n",
    "    with pytest.raises(TypeError):\n",
    "        square(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pytest pytestexample.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTest has lots of other features:\n",
    "\n",
    "* Classes: You can use vanilla classes to group tests\n",
    "* Fixtures: You can get things on a per-test basis, like temporary files, etc.\n",
    "* Markers: You can mark a test as failing, skipped on certain conditions, etc.\n",
    "* Setup/Teardown: Can be done per file, per class, or per test\n",
    "* Configuration: You can set up a configuration file to customize all tests, make your own fixtures, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test driven development\n",
    "\n",
    "Write your tests first, then write the code!\n",
    "\n",
    "* Helps you design the interface before you write the longest part of the code\n",
    "* Is more likely to catch bugs then writing tests when you know the code result\n",
    "* Ensures you do not skip writing the tests\n",
    "* Gives you a target\n",
    "\n",
    "See the great example in [Dive into Python](http://histo.ucsf.edu/BMS270/diveintopython3-r802.pdf) in the Unit Test chapter.\n",
    "\n",
    "I've found in the sciences, test driven development is surprisingly rare. I even do it less than I could."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributions\n",
    "\n",
    "Let's look at generating distributions now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 1: Existing distribution\n",
    "\n",
    "Let's say you are generating a distribution that is quite common: You should be able to find it in SciPy (or even Numpy):\n",
    "\n",
    "* <https://docs.scipy.org/doc/numpy-1.15.1/reference/routines.random.html>\n",
    "* <https://docs.scipy.org/doc/scipy/reference/stats.html>\n",
    "\n",
    "Let's focus on the lognormal distribution:\n",
    "\n",
    "$$\n",
    "p(x) = \\frac{1}{\\sigma x \\sqrt{2\\pi}}\n",
    "e^{-\\frac{\\left(\\ln(x)-\\mu\\right)^2}{2\\sigma^2}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's directly available in Numpy using `np.random.lognormal`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = np.random.lognormal(mean=0, sigma=1, size=100000)\n",
    "plt.hist(vals, bins=\"auto\", range=(0, 10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the one in SciPy, which gives lots of other useful tools. You can use it with a function interface, or an OO interface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = scipy.stats.lognorm.rvs(s=1, size=100000)\n",
    "plt.hist(vals, bins=\"auto\", range=(0, 10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use `loc` and `scale` to adjust the mean that was available in Numpy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also \"freeze\" the distribution into an object (this is the OO interface)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ob = scipy.stats.lognorm(s=1)\n",
    "vals = ob.rvs(size=1000000)\n",
    "plt.hist(vals, bins=\"auto\", range=(0, 10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lots of other methods are available in both interfaces, let's look at one or two:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 10, 1000)\n",
    "y = ob.pdf(x)\n",
    "plt.plot(x, y)\n",
    "plt.title(\"PDF\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 10, 1000)\n",
    "y = ob.cdf(x)\n",
    "plt.plot(x, y)\n",
    "plt.title(\"CDF\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problems with SciPy stats:\n",
    "* Fitting is **very** limited - you can't combine distributions\n",
    "* Not easy to build new distributions\n",
    "* Not all distributions support all methods\n",
    "\n",
    "Other toolkits exist - we'll see more in the fitting lectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 2: Converting an existing distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we didn't know about lognormal. But we do know how to convert an existing distribution to a lognormal, though! The logarithm of a lognormal is normally distributed, so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gauss = np.random.normal(size=100_000)\n",
    "vals = np.exp(gauss)\n",
    "plt.hist(vals, bins=\"auto\", range=(0, 10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 3: Rejection\n",
    "\n",
    "Now we are getting into situations where we do not have a nice existing distribution to use. We will have to start losing events to generate a distribution. Let's look at the most general way first: rejection sampling. For simplicity, we will use the CDF and PDF we already have from SciPy, but you can make your own.\n",
    "\n",
    "A key element you need here is the **maximum** value of the PDF. You must select a cut value larger than this maximum value. You can search for the maximum value beforehand - or if you are lucky, you may know it. We will look at the previous plot and say it's 0.7.\n",
    "\n",
    "<!--\n",
    "xvals = np.random.random_sample(size=100_000) * 10  # 0 to 10\n",
    "yvals = np.random.random_sample(size=100_000) * 0.7 # 0 to .7 (max value)\n",
    "keep = ob.pdf(xvals) > yvals\n",
    "vals = xvals[keep]\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xvals = np.random.random_sample(size=100_000) * 10  # 0 to 10\n",
    "yvals = np.random.random_sample(size=100_000) * 0.7  # 0 to .7 (max value)\n",
    "...\n",
    "\n",
    "plt.hist(vals, bins=\"auto\", range=(0, 10))\n",
    "plt.show()\n",
    "print(f\"Number of samples kept: {len(vals):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 3.b: Non-uniform rejection\n",
    "\n",
    "We can also use a non uniform distribution, were we take the ratio of the two PDFs, and the generation PDF is always above the new PDF (technically, that's what we were doing above)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 4: Using the CDF and binning\n",
    "\n",
    "Let's backtrack a bit and try using the CDF to be a little smarter. If you do not have an analytically invertible CDF, you can make one with bins (your output will only be as accurate as your bins). You need the inverse of the CDF (called the PPF in SciPy).\n",
    "\n",
    "<!--\n",
    "xvals = np.random.random_sample(size=100_000) # 0 to 1\n",
    "vals = ob.ppf(xvals)\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xvals = np.random.random_sample(size=100_000)  # 0 to 1\n",
    "...\n",
    "\n",
    "plt.hist(vals, bins=\"auto\", range=(0, 10))\n",
    "plt.show()\n",
    "print(f\"Number of samples kept: {len(vals):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, assuming we don't have the nice PPF function:\n",
    "\n",
    "<!--\n",
    "xvals = np.random.random_sample(size=100_000) # 0 to 1\n",
    "bin_edges = np.linspace(0,10,101)\n",
    "pdfs = ob.pdf(bin_edges) * (bin_edges[1] - bin_edges[0])\n",
    "cdfs = np.cumsum(pdfs)\n",
    "vals = np.interp(xvals, cdfs, bin_edges)\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xvals = np.random.random_sample(size=100_000)  # 0 to 1\n",
    "...\n",
    "\n",
    "plt.hist(vals, bins=\"auto\", range=(0, 10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question\n",
    "\n",
    "* Why is there a spike at the end of the plot?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "compclass",
   "language": "python",
   "name": "compclass"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
